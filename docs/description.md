# Layers
1. Activation Functions
   * linear - linear
   * step - ?
   * sigmoid - logistic
   * tanh - tanh
   * relu - Rectified linear unit ReLu function
   * prelu - Parametric linear unit ReLu function
2. Various Layer Functions
   * concatenate
   * join
   * split
